{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef02336-77ff-46cb-8f21-e123e32f154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# For DOCX report\n",
    "!pip install python-docx --quiet\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "# ---------- CONFIG: file paths ----------\n",
    "TRADES_PATH = \"trades_with_clusters.csv\"\n",
    "CANDLES_PATH = \"XAUUSD_5min_candles.csv\"\n",
    "REPORT_DOCX_PATH = \"imperium_robustness_improvements_report.docx\"\n",
    "\n",
    "# ---------- CONFIG: column names ----------\n",
    "\n",
    "# In trades_with_clusters.csv\n",
    "TRADES_DATETIME_COL = \"entry_time\"     # entry timestamp\n",
    "TRADES_EXIT_COL = \"exit_time\"          # optional\n",
    "TRADES_PROFIT_COL = \"profit\"\n",
    "TRADES_CLUSTER_COL = \"cluster\"         # 0,1,2,3\n",
    "\n",
    "# ENTRY indicators already computed on the trade row\n",
    "ENTRY_ADX_COL = \"entry_ADX(14)\"\n",
    "ENTRY_RSI_COL = \"entry_RSI(14)\"\n",
    "ENTRY_STOCHK_COL = \"entry_StochK(14)\"\n",
    "ENTRY_BB_UPPER_COL = \"entry_BB_Upper\"\n",
    "ENTRY_BB_MID_COL = \"entry_BB_Mid\"\n",
    "ENTRY_BB_LOWER_COL = \"entry_BB_Lower\"\n",
    "ENTRY_SPREAD_COL_TRADE = \"entry_Spread (Pips)\"  # if you already have it on trades (else use candles)\n",
    "\n",
    "# In XAUUSD_5min_candles.csv\n",
    "CANDLES_DATETIME_COL = \"time\"          # candle timestamp\n",
    "CANDLE_SPREAD_COL = \"spread\"           # or your spread column\n",
    "CANDLE_ATR_COL = \"ATR(14)\"             # or your ATR column\n",
    "CANDLE_VOLUME_COL = \"volume\"           # or tick_volume etc.\n",
    "\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "189ae1e3-450e-49a8-85e5-1829b62ecdff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRADES_EXIT_COL \u001b[38;5;129;01min\u001b[39;00m trades\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     27\u001b[0m     trades[TRADES_EXIT_COL] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(trades[TRADES_EXIT_COL])\n\u001b[0;32m---> 29\u001b[0m candles[CANDLES_DATETIME_COL] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mcandles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCANDLES_DATETIME_COL\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Sort for merge_asof (critical step for time series alignment)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m trades \u001b[38;5;241m=\u001b[39m trades\u001b[38;5;241m.\u001b[39msort_values(TRADES_DATETIME_COL)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "trades = pd.read_csv(TRADES_PATH)\n",
    "candles = pd.read_csv(CANDLES_PATH)\n",
    "\n",
    "# Parse datetimes\n",
    "trades[TRADES_DATETIME_COL] = pd.to_datetime(trades[TRADES_DATETIME_COL])\n",
    "if TRADES_EXIT_COL in trades.columns:\n",
    "    trades[TRADES_EXIT_COL] = pd.to_datetime(trades[TRADES_EXIT_COL])\n",
    "\n",
    "candles[CANDLES_DATETIME_COL] = pd.to_datetime(candles[CANDLES_DATETIME_COL])\n",
    "\n",
    "# Sort for merge_asof\n",
    "trades = trades.sort_values(TRADES_DATETIME_COL).reset_index(drop=True)\n",
    "candles = candles.sort_values(CANDLES_DATETIME_COL).reset_index(drop=True)\n",
    "\n",
    "print(\"Trades shape:\", trades.shape)\n",
    "print(\"Candles shape:\", candles.shape)\n",
    "trades.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c962a03b-b522-4e16-bb4e-5aee248a2be6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'candles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Select needed candle columns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m candle_cols_to_use \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     CANDLES_DATETIME_COL,\n\u001b[1;32m      5\u001b[0m     CANDLE_SPREAD_COL,\n\u001b[1;32m      6\u001b[0m     CANDLE_ATR_COL,\n\u001b[1;32m      7\u001b[0m     CANDLE_VOLUME_COL,\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m candles_ctx \u001b[38;5;241m=\u001b[39m \u001b[43mcandles\u001b[49m[candle_cols_to_use]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     12\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge_asof(\n\u001b[1;32m     13\u001b[0m     trades\u001b[38;5;241m.\u001b[39msort_values(TRADES_DATETIME_COL),\n\u001b[1;32m     14\u001b[0m     candles_ctx\u001b[38;5;241m.\u001b[39msort_values(CANDLES_DATETIME_COL),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Rename candle columns to make it clear they’re at entry\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'candles' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Select needed candle columns\n",
    "candle_cols_to_use = [\n",
    "    CANDLES_DATETIME_COL,\n",
    "    CANDLE_SPREAD_COL,\n",
    "    CANDLE_ATR_COL,\n",
    "    CANDLE_VOLUME_COL,\n",
    "]\n",
    "\n",
    "candles_ctx = candles[candle_cols_to_use].copy()\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    trades.sort_values(TRADES_DATETIME_COL),\n",
    "    candles_ctx.sort_values(CANDLES_DATETIME_COL),\n",
    "    left_on=TRADES_DATETIME_COL,\n",
    "    right_on=CANDLES_DATETIME_COL,\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "# Rename candle columns to make it clear they’re at entry\n",
    "merged = merged.rename(columns={\n",
    "    CANDLE_SPREAD_COL: \"entry_spread_5m\",\n",
    "    CANDLE_ATR_COL: \"entry_ATR_5m\",\n",
    "    CANDLE_VOLUME_COL: \"entry_volume_5m\",\n",
    "})\n",
    "\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3632cc94-4fce-4a30-a281-82be93a2ed76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Time-based features\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentry_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged\u001b[49m[TRADES_DATETIME_COL]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\n\u001b[1;32m      4\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentry_dow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m merged[TRADES_DATETIME_COL]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofweek  \u001b[38;5;66;03m# Monday=0\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_session\u001b[39m(h):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Time-based features\n",
    "merged[\"entry_hour\"] = merged[TRADES_DATETIME_COL].dt.hour\n",
    "merged[\"entry_dow\"] = merged[TRADES_DATETIME_COL].dt.dayofweek  # Monday=0\n",
    "\n",
    "def map_session(h):\n",
    "    if 0 <= h < 8:\n",
    "        return \"asian\"\n",
    "    elif 8 <= h < 12:\n",
    "        return \"london\"\n",
    "    elif 12 <= h < 17:\n",
    "        return \"ny_overlap\"\n",
    "    else:\n",
    "        return \"late_us\"\n",
    "\n",
    "merged[\"entry_session\"] = merged[\"entry_hour\"].apply(map_session)\n",
    "\n",
    "# ATR & spread buckets based on merged data\n",
    "if \"entry_ATR_5m\" in merged.columns:\n",
    "    merged[\"atr_bucket\"] = pd.qcut(\n",
    "        merged[\"entry_ATR_5m\"],\n",
    "        q=3,\n",
    "        labels=[\"low_vol\", \"med_vol\", \"high_vol\"]\n",
    "    )\n",
    "else:\n",
    "    merged[\"atr_bucket\"] = np.nan\n",
    "\n",
    "# Prefer trade-level spread if exists, else use candle spread\n",
    "if ENTRY_SPREAD_COL_TRADE in merged.columns:\n",
    "    merged[\"entry_spread\"] = merged[ENTRY_SPREAD_COL_TRADE]\n",
    "else:\n",
    "    merged[\"entry_spread\"] = merged[\"entry_spread_5m\"]\n",
    "\n",
    "spread_50 = merged[\"entry_spread\"].quantile(0.50)\n",
    "spread_75 = merged[\"entry_spread\"].quantile(0.75)\n",
    "\n",
    "merged[\"spread_bucket\"] = pd.cut(\n",
    "    merged[\"entry_spread\"],\n",
    "    bins=[-np.inf, spread_50, spread_75, np.inf],\n",
    "    labels=[\"low\", \"med\", \"high\"]\n",
    ")\n",
    "\n",
    "merged[[\n",
    "    TRADES_DATETIME_COL, TRADES_CLUSTER_COL, TRADES_PROFIT_COL,\n",
    "    \"entry_hour\", \"entry_dow\", \"entry_session\",\n",
    "    \"entry_ATR_5m\", \"atr_bucket\",\n",
    "    \"entry_spread\", \"spread_bucket\"\n",
    "]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d12ae3ad-fda3-4d8f-aa94-f2b13ce6169e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Baseline per cluster\u001b[39;00m\n\u001b[1;32m     18\u001b[0m baseline_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mmerged\u001b[49m[TRADES_CLUSTER_COL]\u001b[38;5;241m.\u001b[39munique()):\n\u001b[1;32m     20\u001b[0m     df_c \u001b[38;5;241m=\u001b[39m merged[merged[TRADES_CLUSTER_COL] \u001b[38;5;241m==\u001b[39m c]\n\u001b[1;32m     21\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m compute_metrics(df_c)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def compute_metrics(df, profit_col=TRADES_PROFIT_COL):\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return {\"N\": 0, \"win_rate\": np.nan, \"avg_profit\": np.nan, \"total_profit\": 0.0}\n",
    "    wins = (df[profit_col] > 0).sum()\n",
    "    win_rate = wins / n\n",
    "    avg_profit = df[profit_col].mean()\n",
    "    total_profit = df[profit_col].sum()\n",
    "    return {\n",
    "        \"N\": n,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"avg_profit\": avg_profit,\n",
    "        \"total_profit\": total_profit,\n",
    "    }\n",
    "\n",
    "# Baseline per cluster\n",
    "baseline_results = []\n",
    "for c in sorted(merged[TRADES_CLUSTER_COL].unique()):\n",
    "    df_c = merged[merged[TRADES_CLUSTER_COL] == c]\n",
    "    metrics = compute_metrics(df_c)\n",
    "    metrics[\"cluster\"] = c\n",
    "    baseline_results.append(metrics)\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)[\n",
    "    [\"cluster\", \"N\", \"win_rate\", \"avg_profit\", \"total_profit\"]\n",
    "]\n",
    "print(\"Baseline per cluster:\")\n",
    "display(baseline_df)\n",
    "\n",
    "# ---------- Define filters per cluster as lambdas ----------\n",
    "\n",
    "cluster_filters = {\n",
    "    0: {\n",
    "        \"session_asian\": lambda df: df[\"entry_session\"] == \"asian\",\n",
    "        \"monday_only\":   lambda df: df[\"entry_dow\"] == 0,\n",
    "        \"best_hours\":    lambda df: df[\"entry_hour\"].isin([2, 8, 10, 19, 20]),\n",
    "        \"low_med_spread\":lambda df: df[\"spread_bucket\"].isin([\"low\", \"med\"]),\n",
    "        \"rsi_high_terc\": lambda df: df[ENTRY_RSI_COL] >= df[ENTRY_RSI_COL].quantile(2/3),\n",
    "        \"macd_signal_high_terc\": lambda df: df[\"entry_MACD Signal\"] >= df[\"entry_MACD Signal\"].quantile(2/3)\n",
    "                                          if \"entry_MACD Signal\" in df.columns else pd.Series(False, index=df.index),\n",
    "    },\n",
    "    1: {\n",
    "        \"adx_gt_40\":     lambda df: df[ENTRY_ADX_COL] > 40,\n",
    "        \"late_us\":       lambda df: df[\"entry_session\"] == \"late_us\",\n",
    "        \"bb_upper_low_terc\": lambda df: df[ENTRY_BB_UPPER_COL] <= df[ENTRY_BB_UPPER_COL].quantile(1/3),\n",
    "        \"low_med_spread\":lambda df: df[\"spread_bucket\"].isin([\"low\", \"med\"]),\n",
    "    },\n",
    "    2: {\n",
    "        \"adx_gt_40\":     lambda df: df[ENTRY_ADX_COL] > 40,\n",
    "        \"not_monday\":    lambda df: df[\"entry_dow\"] != 0,\n",
    "        \"not_asian\":     lambda df: df[\"entry_session\"] != \"asian\",\n",
    "        \"spread_mid\":    lambda df: df[\"spread_bucket\"] == \"med\",\n",
    "    },\n",
    "    3: {\n",
    "        \"adx_high_terc\": lambda df: df[ENTRY_ADX_COL] >= df[ENTRY_ADX_COL].quantile(2/3),\n",
    "        \"stoch_60_80\":   lambda df: df[ENTRY_STOCHK_COL].between(60, 80),\n",
    "        \"not_thursday\":  lambda df: df[\"entry_dow\"] != 3,\n",
    "        \"low_med_spread\":lambda df: df[\"spread_bucket\"].isin([\"low\", \"med\"]),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Filters defined for clusters:\", cluster_filters.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4244d4-d3a4-4a2f-8724-5658239678be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_filters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m filter_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, filt_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcluster_filters\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     df_c \u001b[38;5;241m=\u001b[39m merged[merged[TRADES_CLUSTER_COL] \u001b[38;5;241m==\u001b[39m c]\n\u001b[1;32m      6\u001b[0m     base_metrics \u001b[38;5;241m=\u001b[39m compute_metrics(df_c)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_filters' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "filter_results = []\n",
    "\n",
    "for c, filt_dict in cluster_filters.items():\n",
    "    df_c = merged[merged[TRADES_CLUSTER_COL] == c]\n",
    "    base_metrics = compute_metrics(df_c)\n",
    "    \n",
    "    for filt_name, filt_fn in filt_dict.items():\n",
    "        try:\n",
    "            mask = filt_fn(df_c)\n",
    "        except Exception as e:\n",
    "            print(f\"Filter {filt_name} on cluster {c} failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df_filt = df_c[mask]\n",
    "        metrics = compute_metrics(df_filt)\n",
    "        metrics.update({\n",
    "            \"cluster\": c,\n",
    "            \"filter\": filt_name,\n",
    "            \"baseline_N\": base_metrics[\"N\"],\n",
    "            \"baseline_wr\": base_metrics[\"win_rate\"],\n",
    "            \"baseline_avg\": base_metrics[\"avg_profit\"],\n",
    "        })\n",
    "        filter_results.append(metrics)\n",
    "\n",
    "filters_df = pd.DataFrame(filter_results)\n",
    "filters_df = filters_df[[\n",
    "    \"cluster\", \"filter\",\n",
    "    \"N\", \"win_rate\", \"avg_profit\", \"total_profit\",\n",
    "    \"baseline_N\", \"baseline_wr\", \"baseline_avg\"\n",
    "]]\n",
    "\n",
    "print(\"All single filters:\")\n",
    "display(filters_df.sort_values([\"cluster\", \"win_rate\"], ascending=[True, False]))\n",
    "\n",
    "print(\"Single filters with N >= 30:\")\n",
    "good_filters_df = filters_df[filters_df[\"N\"] >= 30].copy()\n",
    "display(good_filters_df.sort_values([\"cluster\", \"win_rate\"], ascending=[True, False]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a5c0db-0de0-48e6-9d47-8ccae97e4a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_filters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m combo_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, filt_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcluster_filters\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     df_c \u001b[38;5;241m=\u001b[39m merged[merged[TRADES_CLUSTER_COL] \u001b[38;5;241m==\u001b[39m c]\n\u001b[1;32m      6\u001b[0m     base_metrics \u001b[38;5;241m=\u001b[39m compute_metrics(df_c)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_filters' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "combo_results = []\n",
    "\n",
    "for c, filt_dict in cluster_filters.items():\n",
    "    df_c = merged[merged[TRADES_CLUSTER_COL] == c]\n",
    "    base_metrics = compute_metrics(df_c)\n",
    "    \n",
    "    filt_names = list(filt_dict.keys())\n",
    "    \n",
    "    for combo in combinations(filt_names, 2):  # 2-filter combos\n",
    "        try:\n",
    "            mask = np.ones(len(df_c), dtype=bool)\n",
    "            for fname in combo:\n",
    "                mask &= filt_dict[fname](df_c)\n",
    "        except Exception as e:\n",
    "            print(f\"Combo {combo} on cluster {c} failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df_combo = df_c[mask]\n",
    "        metrics = compute_metrics(df_combo)\n",
    "        metrics.update({\n",
    "            \"cluster\": c,\n",
    "            \"filters\": \"&\".join(combo),\n",
    "            \"baseline_N\": base_metrics[\"N\"],\n",
    "            \"baseline_wr\": base_metrics[\"win_rate\"],\n",
    "            \"baseline_avg\": base_metrics[\"avg_profit\"],\n",
    "        })\n",
    "        combo_results.append(metrics)\n",
    "\n",
    "combo_df = pd.DataFrame(combo_results)\n",
    "combo_df = combo_df[[\n",
    "    \"cluster\", \"filters\",\n",
    "    \"N\", \"win_rate\", \"avg_profit\", \"total_profit\",\n",
    "    \"baseline_N\", \"baseline_wr\", \"baseline_avg\"\n",
    "]]\n",
    "\n",
    "print(\"All 2-filter combinations:\")\n",
    "display(combo_df.sort_values([\"cluster\", \"win_rate\"], ascending=[True, False]))\n",
    "\n",
    "print(\"2-filter combinations with N >= 20:\")\n",
    "good_combo_df = combo_df[combo_df[\"N\"] >= 20].copy()\n",
    "display(good_combo_df.sort_values([\"cluster\", \"win_rate\"], ascending=[True, False]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39eb8053-8d85-49c1-8d4b-3e14e44d35db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Helper: pick top single filter per cluster (by win_rate, N >= 30)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m best_single_per_cluster \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mmerged\u001b[49m[TRADES_CLUSTER_COL]\u001b[38;5;241m.\u001b[39munique()):\n\u001b[1;32m      5\u001b[0m     df_c \u001b[38;5;241m=\u001b[39m good_filters_df[good_filters_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m c]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_c) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Helper: pick top single filter per cluster (by win_rate, N >= 30)\n",
    "best_single_per_cluster = []\n",
    "for c in sorted(merged[TRADES_CLUSTER_COL].unique()):\n",
    "    df_c = good_filters_df[good_filters_df[\"cluster\"] == c]\n",
    "    if len(df_c) == 0:\n",
    "        continue\n",
    "    row = df_c.sort_values(\"win_rate\", ascending=False).iloc[0]\n",
    "    best_single_per_cluster.append(row)\n",
    "\n",
    "best_single_df = pd.DataFrame(best_single_per_cluster)\n",
    "print(\"Best single filter per cluster:\")\n",
    "display(best_single_df)\n",
    "\n",
    "# Plot baseline vs best filter win rate\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(len(best_single_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, best_single_df[\"baseline_wr\"], width, label=\"Baseline WR\")\n",
    "ax.bar(x + width/2, best_single_df[\"win_rate\"], width, label=\"Filtered WR\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"C{int(c)}\" for c in best_single_df[\"cluster\"]])\n",
    "ax.set_ylabel(\"Win Rate\")\n",
    "ax.set_title(\"Baseline vs Best Simple Filter (Win Rate)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot avg profit\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(x - width/2, best_single_df[\"baseline_avg\"], width, label=\"Baseline Avg PnL\")\n",
    "ax.bar(x + width/2, best_single_df[\"avg_profit\"], width, label=\"Filtered Avg PnL\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"C{int(c)}\" for c in best_single_df[\"cluster\"]])\n",
    "ax.set_ylabel(\"Average Profit\")\n",
    "ax.set_title(\"Baseline vs Best Simple Filter (Avg Profit)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb24bd2-70b4-44a9-8ca1-6e0d5141f0db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReport saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Build the report\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m build_docx_report(\u001b[43mbaseline_df\u001b[49m, best_single_df, good_combo_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_df' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def add_table_from_df(document, df, title=None):\n",
    "    if title:\n",
    "        document.add_heading(title, level=2)\n",
    "    table = document.add_table(rows=1, cols=len(df.columns))\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for j, col in enumerate(df.columns):\n",
    "        hdr_cells[j].text = str(col)\n",
    "    for _, row in df.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, col in enumerate(df.columns):\n",
    "            row_cells[j].text = str(row[col])\n",
    "    document.add_paragraph(\"\")  # spacing\n",
    "\n",
    "def build_docx_report(\n",
    "    baseline_df,\n",
    "    best_single_df,\n",
    "    good_combo_df,\n",
    "    output_path=REPORT_DOCX_PATH\n",
    "):\n",
    "    doc = Document()\n",
    "    doc.add_heading(\"Imperium Data Challenge – Robustness & Improvements\", level=1)\n",
    "\n",
    "    doc.add_paragraph(\n",
    "        \"This report summarizes robustness tests and performance improvements \"\n",
    "        \"based on simple, interpretable filters applied to clusters of trades \"\n",
    "        \"for XAUUSD systematic strategies.\"\n",
    "    )\n",
    "\n",
    "    # Baseline\n",
    "    add_table_from_df(doc, baseline_df, title=\"Baseline Performance per Cluster\")\n",
    "\n",
    "    # Best single filters\n",
    "    add_table_from_df(doc, best_single_df, title=\"Best Simple Filters per Cluster (N ≥ 30)\")\n",
    "\n",
    "    # Best combos\n",
    "    good_combo_top = (\n",
    "        good_combo_df\n",
    "        .sort_values([\"cluster\", \"win_rate\"], ascending=[True, False])\n",
    "        .groupby(\"cluster\")\n",
    "        .head(3)  # top 3 combos per cluster\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    add_table_from_df(doc, good_combo_top, title=\"Top 2-Filter Combinations per Cluster (N ≥ 20)\")\n",
    "\n",
    "    # Overfitting discussion\n",
    "    doc.add_heading(\"Overfitting Risk and Robustness Considerations\", level=2)\n",
    "    doc.add_paragraph(\n",
    "        \"Filters were defined using coarse quantile-based thresholds (e.g. low/mid/high terciles) \"\n",
    "        \"and simple time/session rules, to keep the model interpretable and to reduce data-mining risk. \"\n",
    "        \"For each filter or combination, sample size (N) and economic intuition were considered. \"\n",
    "        \"Very high in-sample win rates on small samples (e.g. N < 30) are treated as hypotheses, not \"\n",
    "        \"production-ready rules, and would require validation on a separate out-of-sample period.\"\n",
    "    )\n",
    "    doc.add_paragraph(\n",
    "        \"A next step would be a time-based train/test split (e.g. first 2/3 of the year to design filters, \"\n",
    "        \"last 1/3 to validate) or walk-forward evaluation. In this challenge, the focus is on identifying \"\n",
    "        \"robust patterns such as: trend-following clusters performing best under high-ADX regimes, \"\n",
    "        \"range-trading clusters working better during specific sessions/days, and volatile breakout clusters \"\n",
    "        \"benefiting from simple spread and session filters.\"\n",
    "    )\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"Report saved to: {output_path}\")\n",
    "\n",
    "# Build the report\n",
    "build_docx_report(baseline_df, best_single_df, good_combo_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3dda5-ec7d-4d44-a7fe-78b911fc4d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
